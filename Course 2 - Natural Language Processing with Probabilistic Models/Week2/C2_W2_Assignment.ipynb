{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Parts-of-Speech Tagging (POS)\n",
    "\n",
    "Welcome to the second assignment of Course 2 in the Natural Language Processing specialization. This assignment will develop skills in part-of-speech (POS) tagging, the process of assigning a part-of-speech tag (Noun, Verb, Adjective...) to each word in an input text.  Tagging is difficult because some words can represent more than one part of speech at different times. They are  **Ambiguous**. Let's look at the following example: \n",
    "\n",
    "- The whole team played **well**. [adverb]\n",
    "- You are doing **well** for yourself. [adjective]\n",
    "- **Well**, this assignment took me forever to complete. [interjection]\n",
    "- The **well** is dry. [noun]\n",
    "- Tears were beginning to **well** in her eyes. [verb]\n",
    "\n",
    "Often distinguishing the parts-of-speech of a word in a sentence will help you better understand the meaning of a sentence. This would be critically important in search queries. Identifying the proper noun, the organization, the stock symbol, or anything similar would greatly improve everything ranging from speech recognition to search. By completing this assignment, you will: \n",
    "\n",
    "- Learn how parts-of-speech tagging works\n",
    "- Compute the transition matrix A in a Hidden Markov Model\n",
    "- Compute the transition matrix B in a Hidden Markov Model\n",
    "- Compute the Viterbi algorithm \n",
    "- Compute the accuracy of your own model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages and loading in the data set \n",
    "from utils_pos import get_word_tag, preprocess  \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Sources: \n",
    "This assignment will use two tagged data sets collected from the Wall Street Journal(WSJ). [Here](https://www.clips.uantwerpen.be/pages/mbsp-tags) is an example 'tag-set' or Part of Speech designation describing the two or three letter tag and their meaning. One data set (WSJ-2_21.pos) will be used for training, the other (WSJ-24.pos) for test. The tagged training data has been preprocessed to form a vocabulary (hmm_vocab.txt). The words in the vocabulary are words from the training set that were used two or more times. The vocabulary is augmented with a set of 'unknown word tokens', described below. The training set will be used to create the emission, transmission and tag counts. \n",
    "The test set (WSJ-24.pos) is read in to create 'y'. This contains both the test text and the true tag. The test set has also been preprocessed to remove the tags to form 'test_words.txt'. This is read in and further processed to identify the end of sentences and handle words not in the vocabulary using functions provided in utils_pos.py. This forms the list 'prep', the preprocessed text used to test our  POS taggers.\n",
    "\n",
    "A POS tagger will necessarily encounter words that are not in its datasets. To improve accuracy, these words are further analyzed during preprocessing to extract available hints as to their appropriate tag. For example, the suffix 'ize' is a hint that the word is a verb, as in 'final-ize' or 'character-ize'. A set of unknown-tokens, such as '--unk-verb--' or '--unk-noun--' will replace the unknown words in both the training and test corpus and will appear in the emission, transmission and tag data structures.\n",
    "\n",
    "\n",
    "<img src = \"DataSources1.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation note: For python 3.6 and beyond, dictionaries retain the insertion order. Further, their hash based lookup makes the suitable for rapid membership tests. If _di_ is a dictionary, `key in di` will return `True` if _di_ has a key _key_, else `False`. The dictionary `vocab` will utilize these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the preprocessed test corpus:  34199\n",
      "This is an example of the test_corpus:  The\n",
      "This is an example of your y:  The\tDT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load in the training corpus\n",
    "training_corpus = open(\"WSJ_02-21.pos\", 'r').readlines()           #corpus with tags\n",
    "\n",
    "voc= open(\"hmm_vocab.txt\", 'r').read().split('\\n')\n",
    "vocab = {} # this dictionary has the index of the corresponding words\n",
    "for i, word in enumerate(sorted(voc)): # this gets you the index of the corresponding words. \n",
    "    vocab[word] = i       \n",
    "\n",
    "# load in the test corpus\n",
    "y = open('WSJ_24.pos').readlines()            #corpus with tags\n",
    "_, prep = preprocess(vocab, \"test.words\")     #corpus without tags, preprocessed\n",
    "\n",
    "print('The length of the preprocessed test corpus: ', len(prep))\n",
    "print('This is an example of the test_corpus: ', prep[0])\n",
    "print('This is an example of your y: ', y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Parts-of-speech tagging \n",
    "\n",
    "## Part 1.1 - Training\n",
    "We start with the simplest possible parts-of-speech tagger and we will build up to the state of the art. In this section, you will find the words that are not ambiguous. For example, the word `is` is a verb and it is not ambiguous. In the `WSJ` corpus, $86$% of the token are unambiguous (meaning they have one tag) and around $14\\%$ have more than one tag. \n",
    "\n",
    "<img src = \"pos.png\" style=\"width:400px;height:250px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "Before we start predicting the tags of each word, we will need to compute a few dictionaries that will help us generate the tables. The first dictionary is the `transition_counts` dictionary which computes the number of times each tag happened next to another tag. This dictionary would then be used to compute: \n",
    "$$P(t_i |t_{i-1}) \\tag{1}$$\n",
    "In order for you to compute equation 1, you will create a `transition_counts` dictionary where the keys are `(prev_tag, tag)` and the values are the number of times those two tags appeared in that order. \n",
    "\n",
    "The second dictionary you will compute is the `emission_counts` dictionary. This dictionary would then be used to compute:\n",
    "\n",
    "$$P(w_i|t_i)\\tag{2}$$\n",
    "\n",
    "In other words, you will use it to compute the probability of a word given a tag. In order for you to compute equation 2, you will create an `emission_counts` dictionary where the keys are `(tag, word)` and the values are the number of times that pair showed up in your training set. \n",
    "\n",
    "The last dictionary you will compute is the `tag_counts` dictionary. The keys of this dictionary are the tags and the values is the number of times each tag appeared. \n",
    "\n",
    "**Instructions:** Write a program that takes in the  `training_corpus` and returns the three dictionaries mentioned above `transition_counts`, `emission_counts`, and `tag_counts`. \n",
    "- `emission_counts`: maps (tag, word) to the number of times it happened. \n",
    "- `transition_counts`: maps (prev_tag, tag) to the number of times it has appeared. \n",
    "- `tag_counts`: maps (tag) to the number of times it has occured. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation note: This routine utilises a subclass of *dict*. A standard Python dictionary throws a *KeyError* if you try to access an item with a key that is not currently in the dictionary. In contrast, the *defaultdict* will create an item of the type of the argument, in this case an integer. See [defaultdict](https://docs.python.org/3.3/library/collections.html#defaultdict-objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: create_dictionaries\n",
    "def create_dictionaries(training_corpus, vocab):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        training_corpus: a corpus where each line has a word followed by its tag.\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output: \n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
    "        tag_counts: a dictionary where the keys are the tags and the values are the counts\n",
    "    \"\"\"\n",
    "    m = len(training_corpus)\n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    prev = '--s--' # this is the start state\n",
    "    i = 0 \n",
    "    for line in training_corpus:\n",
    "        i+=1\n",
    "        if i % 50000 == 0:\n",
    "            print(\"line count = \",i)\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        word, tag = get_word_tag(line, vocab)\n",
    "        emission_counts[(tag,word)]+=1\n",
    "        transition_counts[(prev,tag)]+=1\n",
    "        tag_counts[tag]+=1\n",
    "        prev = tag\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return emission_counts, transition_counts, tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line count =  50000\n",
      "line count =  100000\n",
      "line count =  150000\n",
      "line count =  200000\n",
      "line count =  250000\n",
      "line count =  300000\n",
      "line count =  350000\n",
      "line count =  400000\n",
      "line count =  450000\n",
      "line count =  500000\n",
      "line count =  550000\n",
      "line count =  600000\n",
      "line count =  650000\n",
      "line count =  700000\n",
      "line count =  750000\n",
      "line count =  800000\n",
      "line count =  850000\n",
      "line count =  900000\n",
      "line count =  950000\n"
     ]
    }
   ],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "# get all the POS states\n",
    "# CODE REVIEW COMMENT: sorted returns a list so the type conversion is not necessary\n",
    "states = list(sorted(tag_counts.keys()))\n",
    "print(len(states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "46 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine some items in our tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "print(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The 'states' are the Parts-of-speach designations found in the training data. They will also be referred to as 'tags' or POS in this assignment. Above \"NN\" is noun, singlular, while 'NNS' is noun, plural. You can get a more complete description at [Penn Treebank II tag set](https://www.clips.uantwerpen.be/pages/mbsp-tags). In addition, there are helpful tags like '--s--' which indicate a start of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition examples:  [(('--s--', 'IN'), 5050), (('IN', 'DT'), 32364), (('DT', 'NNP'), 9044)]\n",
      "emission examples:  [(('DT', 'any'), 721), (('NN', 'decrease'), 7), (('NN', 'insider-trading'), 5)]\n",
      "ambiguous word example: \n",
      "('RB', 'back') 304\n",
      "('VB', 'back') 20\n",
      "('RP', 'back') 84\n",
      "('JJ', 'back') 25\n",
      "('NN', 'back') 29\n",
      "('VBP', 'back') 4\n"
     ]
    }
   ],
   "source": [
    "print(\"transition examples: \", list(transition_counts.items())[:3])\n",
    "print(\"emission examples: \", list(emission_counts.items())[200:203])\n",
    "print(\"ambiguous word example: \")\n",
    "for tup,cnt in emission_counts.items():\n",
    "    if tup[1] == 'back': print (tup, cnt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "transition examples:  [(('--s--', 'IN'), 5050), (('IN', 'DT'), 32364), (('DT', 'NNP'), 9044)]  \n",
    "emission examples:  [(('DT', 'any'), 721), (('NN', 'decrease'), 7), (('NN', 'insider-trading'), 5)]   \n",
    "('RB', 'back')&nbsp; 304  \n",
    "('VB', 'back')&nbsp; 20  \n",
    "('RP', 'back')&nbsp; 84  \n",
    "('JJ', 'back')&nbsp; 25  \n",
    "('NN', 'back')&nbsp; 29  \n",
    "('VBP', 'back')&nbsp; 4  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 - Testing -\n",
    "\n",
    "Now you will test the accuracy of your parts-of-speech tagger using your `emission_counts` dictionary. Given your preprocessed test corpus `prep`, you will assign a parts-of-speech tag to every word in that corpus. Using the original tagged test corpus `y`, you will then compute what percent of the tags you got correct. \n",
    "\n",
    "**Instructions:** Implement `predict_pos` that computes the accuracy of your model. This is a warmup exercise. To assign a part of speech to a word, simply assign the most frequent POS for that word in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: predict_pos\n",
    "\n",
    "def predict_pos(prep, y, emission_counts, vocab, states):\n",
    "    '''\n",
    "    Input: \n",
    "        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n",
    "        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n",
    "        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "        states: a sorted list of all possible tags for this assignment\n",
    "    Output: \n",
    "        accuracy: Number of times you classified a word correctly\n",
    "    '''\n",
    "    \n",
    "    num_correct = 0\n",
    "    all_words = set(emission_counts.keys())\n",
    "    total = len(y)\n",
    "    for word, y_tup in zip(prep, y): \n",
    "        if not y_tup.split():\n",
    "            continue \n",
    "        _, true_label = y_tup.split()\n",
    "        count_final = 0\n",
    "        pos_final = ''\n",
    "        if word in vocab:\n",
    "            for pos in states:\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "                key = (pos, word)\n",
    "                if key in all_words: \n",
    "                    freq = emission_counts[key]\n",
    "                    if freq > count_final:\n",
    "                        count_final = freq\n",
    "                        pos_final = pos\n",
    "            if pos_final == true_label:\n",
    "                num_correct +=1\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return num_correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8888563993099213\n"
     ]
    }
   ],
   "source": [
    "p = predict_pos(prep, y, emission_counts, vocab, states)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:** 0.8888563993099213\n",
    "\n",
    "88.9% is really good for this warm up exercise. With some dynamic programming, you should be able to get **95% accuracy.** Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Hidden Markov Models for POS\n",
    "\n",
    "In this part you will build something more context specific. Concretely, you will be implementing a Hidden Markov Model (HMM) with a Viterbi decoder, one of the most commonly used algorithms in Natural Language Processing which acts as a foundation to many deeplearning techniques you will later see in the specialization. In addition to parts-of-speech tagging, it is used in speech recognition, speech synthesis, etc... By completing this part of the assignment you will get a 95% accuracy on the same dataset you used in part 1.\n",
    "\n",
    "The Markov Model contains a number of states and the probabily of transition between those states. In our case, the states are the parts-of-speech. It utlizes a transition matrix, `A`. A Hidden Markov Model adds an observation or emission matrix `B` which describes the probability of a visible observation when in a particular state. In our case, the emissions are the words in the corpus, the state, which is hidden, is the tag of that word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1 Generating Matrices\n",
    "\n",
    "### Creating the A transition probabilities matrix\n",
    "Now that you have your emission_counts, transition_counts, and tag_counts, you will start implementing the Hidden Markov Model. This will allow you to quickly construct the `A` transition probabilities matrix and the `B` emission probabilities matrix. You will also use some smoothing when computing them. Here is an example of what the `A` transition matrix would look like (simplified to 5 tags for viewing. It is 46x46 in this assignment.): \n",
    "<!img src = \"A_PROBS.png\" style=\"width:500px;height:200px;\"/>\n",
    "<p style='text-align: center;'> <b>A Transitions Probability Matrix (subset)</b>  </p>\n",
    "\n",
    "|**A**  |...|         RBS  |          RP  |         SYM  |      TO  |          UH|...\n",
    "| --- ||---:-------------| ------------ | ------------ | -------- | ---------- |----\n",
    "|**RBS**  |...|2.217069e-06  |2.217069e-06  |2.217069e-06  |0.008870  |2.217069e-06|...\n",
    "|**RP**   |...|3.756509e-07  |7.516775e-04  |3.756509e-07  |0.051089  |3.756509e-07|...\n",
    "|**SYM**  |...|1.722772e-05  |1.722772e-05  |1.722772e-05  |0.000017  |1.722772e-05|...\n",
    "|**TO**   |...|4.477336e-05  |4.472863e-08  |4.472863e-08  |0.000090  |4.477336e-05|...\n",
    "|**UH**  |...|1.030439e-05  |1.030439e-05  |1.030439e-05  |0.061837  |3.092348e-02|...\n",
    "| ... |...| ...          | ...          | ...          | ...      | ...        | ...\n",
    "\n",
    "Note that the matrix above was computed with smoothing. Each cell gives you the probability to go from one part of speech to another. In other words to go from parts-of-speech `TO` to `RP`, there is a 4.48e-8 chance. As you might have guessed, the sum of each row has to equal 1. The smoothing was done as follows: \n",
    "\n",
    "$$ P(t_i | t_{i-1}) = \\frac{C(t_{i-1}, t_{i}) + \\alpha }{C(t_{i-1}) +\\alpha * N}\\tag{3}$$\n",
    "\n",
    "Where $N$ is the total number of tags, $C$ are the counts in transmission_counts and alpha is a smoothing parameter.\n",
    "\n",
    "\n",
    "**Instructions:** Implement the `create_transition_matrix` below for all tags. Your task is to output a matrix that computes equation 3 for each cell in matrix `A`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: create_transition_matrix\n",
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    ''' \n",
    "    Input: \n",
    "        alpha: number used for smoothing\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        transition_counts: \n",
    "    Output:\n",
    "        A: matrix of dimension (num_tags,num_tags)\n",
    "    '''\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    num_tags = len(all_tags)\n",
    "    A = np.zeros((num_tags,num_tags))\n",
    "    trans_keys = set(transition_counts.keys())\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    for i in range(num_tags):\n",
    "        deno = tag_counts[all_tags[i]] + alpha * num_tags\n",
    "        for j in range(num_tags):\n",
    "            A[i,j] = (transition_counts[(all_tags[i],all_tags[j])]+ alpha)/ deno\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.039972966503809e-06\n",
      "0.16910191896905374\n",
      "              RBS            RP           SYM        TO            UH\n",
      "RBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06\n",
      "RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07\n",
      "SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05\n",
      "TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05\n",
      "UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "# Testing your function\n",
    "print(A[0,0])\n",
    "print(A[3,1])\n",
    "A_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\n",
    "print(A_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "- 7.039972966503809e-06\n",
    "- 0.16910191896905374\n",
    "- And \n",
    "\n",
    "|  -  |         RBS  |          RP  |         SYM  |      TO  |          UH|\n",
    "| --- | ------------ | ------------ | ------------ | -------- | ---------- |\n",
    "|RBS  |2.217069e-06  |2.217069e-06  |2.217069e-06  |0.008870  |2.217069e-06|\n",
    "|RP   |3.756509e-07  |7.516775e-04  |3.756509e-07  |0.051089  |3.756509e-07|\n",
    "|SYM  |1.722772e-05  |1.722772e-05  |1.722772e-05  |0.000017  |1.722772e-05|\n",
    "|TO   |4.477336e-05  |4.472863e-08  |4.472863e-08  |0.000090  |4.477336e-05|\n",
    "|UH   |1.030439e-05  |1.030439e-05  |1.030439e-05  |0.061837  |3.092348e-02|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the B emission probabilities matrix\n",
    "\n",
    "Now you will create the `B` transition matrix which computes the emission probability. You will use smoothing as defined below: \n",
    "\n",
    "\n",
    "$$P(w_i | t_i) = \\frac{C(t_i, word_i)+ \\alpha}{C(t_{i}) +\\alpha * N}\\tag{4}$$\n",
    "\n",
    "Where $C(t_i, word_i)$ is the number of times $word_i$ was associated with $tag_i$ in the training data, $C(t_i)$ is the number of times $tag_i$ was in the training data, N is the number of words in the vocabulary and alpha is a smoothing parameter. Your matrix `B` is of dimension (num_tags, N), where num_tags is the number of possible parts-of-speech. Here is an example of the matrix, only a subset of tags and words are shown: \n",
    "<p style='text-align: center;'> <b>B Emissions Probability Matrix (subset)</b>  </p>\n",
    "\n",
    "|**B**| ...|          725 |     adroitly |    engineers |     promoted |      synergy| ...|\n",
    "|----|----|--------------|--------------|--------------|--------------|-------------|----|\n",
    "|**CD**  | ...| **8.201296e-05** | 2.732854e-08 | 2.732854e-08 | 2.732854e-08 | 2.732854e-08| ...|\n",
    "|**NN**  | ...| 7.521128e-09 | 7.521128e-09 | 7.521128e-09 | 7.521128e-09 | **2.257091e-05**| ...|\n",
    "|**NNS** | ...| 1.670013e-08 | 1.670013e-08 |**4.676203e-04** | 1.670013e-08 | 1.670013e-08| ...|\n",
    "|**VB**  | ...| 3.779036e-08 | 3.779036e-08 | 3.779036e-08 | 3.779036e-08 | 3.779036e-08| ...|\n",
    "|**RB**  | ...| 3.226454e-08 | **6.456135e-05** | 3.226454e-08 | 3.226454e-08 | 3.226454e-08| ...|\n",
    "|**RP**  | ...| 3.723317e-07 | 3.723317e-07 | 3.723317e-07 | **3.723317e-07** | 3.723317e-07| ...|\n",
    "| ...    | ...|     ...      |     ...      |     ...      |     ...      |     ...      | ...|\n",
    "\n",
    "<!img src = \"B_probs.png\" style=\"width:500px;height:200px;\"/>\n",
    "\n",
    "**Instructions:** Implement the `create_emission_matrix` below that computes the `B` emission probabilities matrix. Your function takes in $\\alpha$, the smoothing parameter, `tag_counts`, which is a dictionary mapping each tag to its respective count, the `emission_counts` dictionary where the keys are (tag, word) and the values are the counts. Your task is to output a matrix that computes equation 4 for each cell in matrix `B`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: create_emission_matrix\n",
    "\n",
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        alpha: tuning parameter used in smoothing \n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output:\n",
    "        B: a matrix of dimension (num_tags, len(vocab))\n",
    "    '''\n",
    "    '''    for i in range(num_tags):\n",
    "        deno = tag_counts[all_tags[i]] + alpha * num_tags\n",
    "        for j in range(num_words):\n",
    "            B[i,j] = (emission_counts[(all_tags[i],vocab[j])] + alpha)/deno\n",
    "    '''\n",
    "    num_tags = len(tag_counts)\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    num_words = len(vocab)\n",
    "    B = np.zeros((num_tags, num_words))\n",
    "    emis_keys = set(list(emission_counts.keys()))\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for first_part in range(num_tags):\n",
    "        for second_part in range(num_words):\n",
    "            count = 0\n",
    "            current_tuple = (all_tags[first_part], vocab[second_part])\n",
    "            if current_tuple in emis_keys:\n",
    "                count = emission_counts[current_tuple]\n",
    "            B[first_part,second_part] = (count + alpha) / (tag_counts[current_tuple[0]] + alpha * num_words)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.032199882975323e-06\n",
      "7.195398974080014e-07\n",
      "              725      adroitly     engineers      promoted       synergy\n",
      "CD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08\n",
      "NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\n",
      "NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\n",
      "VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\n",
      "RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\n",
      "RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07\n"
     ]
    }
   ],
   "source": [
    "# creating your emission probability matrix. this takes a few minutes to run. \n",
    "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
    "print(B[0,0])\n",
    "print(B[3,1])\n",
    "cidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n",
    "cols = [vocab[a] for a in cidx]\n",
    "rvals =['CD','NN','NNS', 'VB','RB','RP']\n",
    "rows = [states.index(a) for a in rvals]\n",
    "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
    "print(B_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "- 6.032199882975323e-06\n",
    "- 7.195398974080014e-07   and\n",
    "\n",
    "|  \\ |          725 |     adroitly |    engineers |     promoted |      synergy\n",
    "|----|--------------|--------------|--------------|--------------|-------------|\n",
    "|CD  | 8.201296e-05 | 2.732854e-08 | 2.732854e-08 | 2.732854e-08 | 2.732854e-08\n",
    "|NN  | 7.521128e-09 | 7.521128e-09 | 7.521128e-09 | 7.521128e-09 | 2.257091e-05\n",
    "|NNS | 1.670013e-08 | 1.670013e-08 | 4.676203e-04 | 1.670013e-08 | 1.670013e-08\n",
    "|VB  | 3.779036e-08 | 3.779036e-08 | 3.779036e-08 | 3.779036e-08 | 3.779036e-08\n",
    "|RB  | 3.226454e-08 | 6.456135e-05 | 3.226454e-08 | 3.226454e-08 | 3.226454e-08\n",
    "|RP  | 3.723317e-07 | 3.723317e-07 | 3.723317e-07 | 3.723317e-07 | 3.723317e-07\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Viterbi Algorithm and Dynamic Programming\n",
    "\n",
    "In this part of the assignment you will implement the Viterbi algorithm which makes use of dynamic programming. Specifically, you will use your two matrices, `A` and `B` to compute the Viterbi algorithm. We have decomposed this process into three main steps for you. \n",
    "\n",
    "* **Initialization** - In this part you initialize the best_paths and best_probabilities matrices that you will be populating in feed_forward.\n",
    "* **Feed forward** - At each step, you calculate the probability of each path happening and the best paths up to that point. \n",
    "* **Feed backward**: This allows you to find the best path with the highest probabilities. \n",
    "\n",
    "## Part 3.1:  Initialization \n",
    "\n",
    "You will start by initializing two matrices of the same dimension. \n",
    "\n",
    "- best_probs: Each cell contains the probability of going from one tag to that word in the corpus.\n",
    "\n",
    "- best_paths: A matrix that helps you trace through the best possible path in the corpus. \n",
    "\n",
    "**Instructions**: Write a program below that initializes the `best_probs` and the `best_paths` matrix. Both matrices will be initilized to zero except for the first column of `best_probs`.  That column is initialized assuming the first word of the corpus was preceeded by a start token(\"--s--\"). This allows you to reference the A matrix for the transition probablity.\n",
    "The values in column zero are set to: \n",
    "\n",
    "$ if A[s_{idx}, i] <> 0 : best\\_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]$\n",
    "\n",
    "$ if A[s_{idx}, i] == 0 : best\\_probs[i,0] = float('-inf')$\n",
    "\n",
    "$vocab[corpus[0]]$ gives you the column index into the `B` matrix of the first word of the corpus. Taking the log is an implementation hack. If you were to just multiply $A[s_{idx}, i] \\times B[i, vocab[corpus[0]] $ then you might get very small numbers as your corpus gets larger. The A=0 case has special handling to avoid taking the log of 0. \n",
    "\n",
    "Utilize [math.log](https://docs.python.org/3/library/math.html) to compute the natural logarithm.\n",
    "\n",
    "The example below shows the initialization assuming the corpus starts with the phrase \"Loss tracks upward\".\n",
    "\n",
    "<img src = \"Initialize4.PNG\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: initialize\n",
    "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        states: a list of all possible parts-of-speech\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        A: Transition Matrix of dimension (num_tags, num_tags)\n",
    "        B: Emission   Matrix of dimension (num_tags, len(vocab))\n",
    "        corpus: a sequence of words whose POS is to be identified in a list \n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output:\n",
    "        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n",
    "        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n",
    "    '''\n",
    "    num_tags = len(tag_counts)\n",
    "    best_probs = np.zeros((num_tags, len(corpus)))\n",
    "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
    "    s_idx = states.index(\"--s--\")\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(num_tags):\n",
    "        if A[s_idx,i]:\n",
    "            best_probs[i,0] = math.log(A[s_idx,i]) + math.log(B[i,vocab[corpus[0]]])\n",
    "        else:\n",
    "            best_probs[i,0] = float('-inf')\n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-22.60982633354825\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(best_probs[0,0]) # -22.60.....\n",
    "print(best_paths[2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:** \n",
    "* -22.60982633354825\n",
    "* 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.2 Viterbi Forward\n",
    "\n",
    "In this part of the assignment, you will implement the viterbi forward segment. In other words, you will populate your `best_probs` and `best_paths` matrices.\n",
    "\n",
    "The Viterbi forward algorithm will walk forward through the corpus and for each word compute a probability for each possible tag. Unlike our previous algorithm this will include the path up to that word,tag combination. \n",
    "An example makes this more clear. Note, in example, only a subset of states are shown due to space limitiations. In the diagram below, the first word is already initialized. The algorithm will compute a probability for each of the potential tags in the second and future words. For example, to compute the probability that the tag of 'tracks' is verb, 3rd person singular present (VBZ), highlighed in orange below, it will examine each of the paths from the tags of 'Loss' and choose the most likely.  An example of the calculation for **one** of those paths is the path from (\"Loss\", NN). The log of the probability of the path to and including 'Loss\" being a noun (NN) is -14.32. We add to that the log of the probability of NN transitioning to VBS from the A matrix, circled in the diagram, log(4.37e-02). To that we add the log of the probability that the tag VBS would have an 'emission' 'tracks', log(4.61e-4) for a total of -25.13. This will turn out to the the most likely, but all other paths from 'Loss' are examined as well. The most likely path is stored in the best_path table - shown in orange. \n",
    "The formula's to compute probability and path for corpus[i], current tag j and previous tag k is:\n",
    "\n",
    "`` prob = best_probs[k, i-1] + log(A[k, j]) + log(B[j, vocab[corpus[i]]``\n",
    "\n",
    "`` path = k ``\n",
    "\n",
    "\n",
    "**Instructions:** Implement the viterbi forward algorithm and store the best_path and best_prob for every possible tag for each word in the matrices `best_probs` and `best_tags` using the pseudo code below.\n",
    "\n",
    "`for all corpus (1 in diagram)  \n",
    "    for each tag type (2 in diagram)   \n",
    "        for each input probability from previous entry (3in diagram)   \n",
    "            compute the probability using formula above   \n",
    "            retain the highest probabilty computed   \n",
    "            set best_probs to that values  \n",
    "            set best_paths to the index of the previous entry that produced the highest probability `\n",
    "\n",
    "Utilize [math.log](https://docs.python.org/3/library/math.html) to compute the natural logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Forward4.PNG\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: viterbi_forward\n",
    "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        A, B: The transiton and emission matrices respectively\n",
    "        test_corpus: a list containing a preprocessed corpus\n",
    "        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index \n",
    "    Output: \n",
    "        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
    "    '''\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    for i in range(1, len(test_corpus)): # for every word in the corpus\n",
    "        if i % 5000 == 0:\n",
    "            print(\"Words processed: {:>8}\".format(i))\n",
    "            \n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        for each_tag in range(num_tags): \n",
    "            \n",
    "            best_prob = float(\"-inf\")\n",
    "            best_path = []\n",
    "            \n",
    "            for each_input in range(num_tags): \n",
    "                \n",
    "                prob = best_probs[each_input, i - 1] + math.log(A[each_input, each_tag]) + math.log(B[each_tag][vocab[test_corpus[i]]])\n",
    "                \n",
    "                if prob > best_prob:\n",
    "                    best_prob = prob\n",
    "                    best_path = each_input\n",
    "                    \n",
    "            best_probs[each_tag, i] = best_prob\n",
    "            best_paths[each_tag, i] = best_path\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words processed:     5000\n",
      "Words processed:    10000\n",
      "Words processed:    15000\n",
      "Words processed:    20000\n",
      "Words processed:    25000\n",
      "Words processed:    30000\n"
     ]
    }
   ],
   "source": [
    "# this will take a few minutes to run => processes ~ 30,000 words\n",
    "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-24.78215632717346\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# Testing this function \n",
    "print(best_probs[0,1]) \n",
    "print(best_paths[0,4]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "* -24.78215632717346\n",
    "* 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.3 Viterbi backward\n",
    "\n",
    "Now you will implement the Viterbi backward algorithm which allows you to get the predictions from the `best_paths` and the `best_probs` matrices you have already implemented. \n",
    "\n",
    "You have filled in the `best_paths` and the `best_probs` matrices in the forward path. The example below,  shows how to proceed.  You select the the most likely entry for the last word in the corpus, 'upward' in the `best_probs` table. It is `RB`, an adverb, at offset 28. Store this in the last entry of `pred`. Select offset 28 of the last entry of `best_paths`, 40. This points back to 'VBZ' (verb, 3rd person singular present) for the word 'tracks'. Following links backward to the start, each word can be assigned its most likely POS. These are stored in the correpsonding etnry of `pred`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Backwards5.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** Implement the `viterbi_backward` algorithm that returns a list of predictions.  \n",
    "The indexing can be a bit confusing. Note in the small example above m = 3, so the last best_probs/paths entry is column 2 or m-1.  \n",
    "_In Step 1:_       \n",
    "Loop through all the rows/tags in the last entry of best_probs and find the row/tag with the maximum value.\n",
    "Convert the index to a tag using `states`.  \n",
    "In our small example above,  \n",
    "`z[2] = 28` and\n",
    "`pred[2] = states(z[2])` , states(z[2]) is 'RB' in this case.\n",
    "\n",
    "_In Step 2_:  \n",
    "Starting at the last entry of best_paths, use the index from step 1 as the start and then follow the pointers back to the start. Record the index's at each step and convert the index to a tag using `states`. \n",
    "\n",
    "In our small example above, in Step 2, the first iteration, you would read best_paths[:,2] and fill in z[1]  \n",
    "`z[1] = best_paths[z[2],2]`  \n",
    "The small test following the routine prints the last few words of the corpus and their states to aid in debug.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: viterbi_backward\n",
    "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
    "    '''\n",
    "    This function returns the best path.\n",
    "    \n",
    "    '''\n",
    "    m = best_paths.shape[1] \n",
    "    z = [None] * m\n",
    "    num_tags = best_probs.shape[0]\n",
    "    argmax = best_probs[0, m - 1]\n",
    "    pred = [None] * m\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(num_tags):\n",
    "        if best_probs[i,m-1] > argmax:\n",
    "            argmax = best_probs[i,m-1]\n",
    "            z[m-1] = i\n",
    "    pred[m-1] = states[z[m-1]]\n",
    "    for i in reversed(range(m-1)):\n",
    "        z[i] = best_paths[z[i+1],i+1]\n",
    "        pred[i] = states[z[i]]\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for pred[-7:m-1] is: \n",
      " ['see', 'them', 'here', 'with', 'us', '.'] \n",
      " ['VB', 'PRP', 'RB', 'IN', 'PRP', '.'] \n",
      "\n",
      "The prediction for pred[0:8] is: \n",
      " ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN'] \n",
      " ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken']\n"
     ]
    }
   ],
   "source": [
    "pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
    "m=len(pred)\n",
    "print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
    "print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**   \n",
    " <span style=\"font-family:Courier\">\n",
    "The prediction for prep[-7:m-1] is:  \n",
    " ['see', 'them', 'here', 'with', 'us', '.']  \n",
    " ['VB', 'PRP', 'RB', 'IN', 'PRP', '.']   \n",
    "The prediction for pred[0:8] is:    \n",
    " ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN']   \n",
    " ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken'] \n",
    "</span>\n",
    "\n",
    "Now you just have to compare the predicted labels to the true labels and you are done! You can then see the accuracy on the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Predicting on a data set\n",
    "\n",
    "In this part of the assignment you compute the accuracy of your prediction with the true `y` value. Your pred is a list of predictions corresponding to the predicted words of your `test_corpus`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The third word is:', prep[3])\n",
    "print('Your prediction is:', pred[3])\n",
    "print('Your corresponding y is: ', y[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now implement a function to compute the accuracy of your predictions.\n",
    "\n",
    "**Instructions:** Implement a function that computes the accuracy of your predictions. to split y into the word and its tag you can use `y.split()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: compute_accuracy\n",
    "def compute_accuracy(pred, y):\n",
    "    '''\n",
    "    Input: \n",
    "        pred: a list of the predicted parts-of-speech \n",
    "        y: a list of lines where each word is separated by a '\\t' (i.e. word \\t tag)\n",
    "    Output: \n",
    "        \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    for prediction, y in zip(pred, y):\n",
    "        if not y.split():\n",
    "            continue\n",
    "            \n",
    "        ### START CODE HERE ###\n",
    "        word, tag = y.split()\n",
    "        if tag == prediction:\n",
    "            num_correct += 1 \n",
    "        total += 1        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return num_correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.953063647155511"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:** 0.953063647155511\n",
    "\n",
    "Congratulations you were able to classify the parts-of-speech with 95% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take aways and overview\n",
    "\n",
    "In this assignment you learnt about parts-of-speech tagging. You have computed POS by going forward in a corpus. Some implementations use bidirectional tagging, meaning knowing the previous word and the next word would tell you more about the POS instead of just knowing the previous word. But of course, if you can implement this unidirectional approach, it lays the foundation to other POS taggers used in industry, (which are just a little bit more complicated than what you have just implemented). This assignment is critical to understanding other POS tagging methods. With this implementation you managed to get 95% accuracy. Congratulations and see you next week where we introduce **Ngram Language Models** which are also extremely useful for speech recognition and other things you will later see in the specialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "- [\"Speech and Language Processing\", Dan Jurafsky and James H. Martin](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "- We would like to thank Melanie Tosik for her help and inspiration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional:Create and test your own mini-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify sentence to test a small corpus\n",
    "sentence = \"Loss tracks upward\".split()\n",
    "\n",
    "srvals = {\"--s--\"} # start with start (set)\n",
    "for s in states:\n",
    "    for w in sentence:\n",
    "        if emission_counts[(s,w)] > 0:\n",
    "            print(s,w)\n",
    "            srvals.add(s)\n",
    "print(srvals)\n",
    "rvals = sorted(list(srvals))\n",
    "rcvals = [\"({}){}\".format(states.index(i),i) for i in rvals]\n",
    "print(\"Interresting states: \",rcvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display interresting Parts of B\n",
    "cols = [list(vocab).index(a) for a in sentence]\n",
    "rows = [states.index(a) for a in rvals]\n",
    "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rcvals, columns = sentence )\n",
    "pd.options.display.float_format = '{:,.2e}'.format\n",
    "print(B_sub)\n",
    "pd.reset_option(\"display.float_format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display interresting parts of A\n",
    "rows = [states.index(a) for a in rvals]\n",
    "A_sub = pd.DataFrame(A[np.ix_(rows,rows)], index=rcvals, columns =rvals )\n",
    "pd.options.display.float_format = '{:,.2e}'.format\n",
    "print(A_sub)\n",
    "pd.reset_option(\"display.float_format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize, display results\n",
    "mini_best_probs, mini_best_paths = initialize(states, tag_counts, A, B,sentence,vocab)\n",
    "\n",
    "rows = [states.index(a) for a in rvals]\n",
    "bpr_sub = pd.DataFrame(mini_best_probs[np.ix_(rows,range(len(sentence)))], index=rcvals, columns =sentence )\n",
    "bpa_sub = pd.DataFrame(mini_best_paths[np.ix_(rows,range(len(sentence)))], index=rcvals, columns =sentence )\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "print(bpr_sub)\n",
    "print(bpa_sub)\n",
    "print(sentence)\n",
    "pd.reset_option(\"display.float_format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run forward, backward, display results\n",
    "mini_best_probs, mini_best_paths = viterbi_forward(A, B, sentence, mini_best_probs, mini_best_paths,vocab)\n",
    "mini_pred = viterbi_backward(mini_best_probs, mini_best_paths, sentence, states)\n",
    "rows = [states.index(a) for a in rvals]\n",
    "bpr_sub = pd.DataFrame(mini_best_probs[np.ix_(rows,range(len(sentence)))], index=rcvals, columns =sentence )\n",
    "bpa_sub = pd.DataFrame(mini_best_paths[np.ix_(rows,range(len(sentence)))], index=rcvals, columns =sentence )\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "print(bpr_sub)\n",
    "print(bpa_sub)\n",
    "print(sentence)\n",
    "print(mini_pred)\n",
    "pd.reset_option(\"display.float_format\")"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC2-2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
